{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "bdb2f4ba1189940c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import tiktoken  # Tokenizer-Bibliothek für GPT-2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "id": "39c18197745f17a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Tokenisierung:\n",
    "- Tokenisierung ist der Prozess der Umwandlung von Text in kleinere Einheiten (Tokens).\n",
    "- Tokens können Zeichen (\"a\", \"b\"), Wörter (\"Hallo\", \"Baum\") oder Subwörter (\"Ha\", \"Ba\") sein.\n",
    "- LLMs arbeiten mit Token-Sequenzen anstelle von Rohtext → Tokenisierung ist daher das Vorbereiten des Textes für die Eingabe in das LLM :)\n",
    "\n",
    "\n"
   ],
   "id": "821f5ac9591f9c88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.1 Datensatz erstellen:",
   "id": "6dccab5cb24e5378"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DatasetForGPT(Dataset):\n",
    "    def __init__(self, max_length, stride, tokenizer, txt):\n",
    "        \"\"\"\n",
    "        Erstellt ein Dataset aus einem Text für ein GPT-basiertes Modell.\n",
    "\n",
    "        Parameter:\n",
    "        - txt: Der Eingabetext als String. (z.B. ein Opensource Buch der Seite Projekt Gutenberg https://www.projekt-gutenberg.org/)\n",
    "        - tokenizer: Der GPT2-Tokenizer zur Tokenisierung des Textes.\n",
    "        - max_length: Maximale Länge einer Token-Sequenz.\n",
    "        - stride: Schrittweite für die Erstellung überlappender Sequenzen.\n",
    "        \"\"\"\n",
    "        self.input_ids = []  # Liste zur Speicherung der Eingabe-Tokens\n",
    "        self.target_ids = []  # Liste zur Speicherung der Ziel-Tokens (verschobene Sequenz)\n",
    "\n",
    "        # Den gesamten Text in Tokens umwandeln\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Erzeuge überlappende Sequenzen aus den Token-IDs (siehe Erklärung 1)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]  # Eingabesequenz\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]  # Zielsequenz (verschoben um 1 Token -> siehe Bild 1: Erklärung 1 )\n",
    "\n",
    "            # Speichern der Tensor-Repräsentation der Sequenzen\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Gibt die Anzahl der Trainingsbeispiele zurück.\"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Gibt ein Trainingsbeispiel bestehend aus (Eingabe, Ziel) zurück.\"\"\"\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ],
   "id": "6cc5a953243b551d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Erklärung 1\n",
    "Das bedeutet:\n",
    "- Das Modell bekommt eine Eingabesequenz (input_chunk), z. B. \"LLMs learn to predict one\"\n",
    "- Die Zielsequenz (target_chunk) ist dann dieselbe Sequenz, aber um ein Token nach rechts verschoben, sodass das Modell lernen soll, das nächste Token vorherzusagen.\n",
    "\n",
    "→ Genau wie im Bild (Quelle: Raschka 2025)\n",
    "###### Bild 1 Sliding Window Approach\n",
    "![Image 2 Sliding Window Approach](images/Image1_sliding_window_approach.png)\n",
    "\n",
    "Das Modell sieht nur den bisherigen Kontext (blau markiert).\n",
    "Das Modell soll das nächste Wort (rot markiert) vorhersagen.\n",
    "Es kann zukünftige Wörter nicht direkt sehen, sondern muss sie aus den bisherigen Token ableiten. Während des Trainings bekommt das Modell eine Sequenz (input_chunk) als Eingabe und versucht, das nächste Token (target_chunk) vorherzusagen. Durch viele Wiederholungen lernt das Modell dann grammatische Strukturen, Satzbedeutungen und sogar komplexe Zusammenhänge."
   ],
   "id": "968507db476bb9a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1.2 Dataloader erzeugen:\n",
    "\n",
    "Der DataLoader hilft, die Daten effizient für das LLM bereitzustelle.\n"
   ],
   "id": "b071da1ed1ccd5e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_dataloader(txt, batch_size=4, max_length=256,\n",
    "                      stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    \"\"\"\n",
    "    Erstellt einen DataLoader für das Training eines LLMs.\n",
    "\n",
    "    Parameter:\n",
    "    - txt: Eingabetext als String.\n",
    "    - batch_size: Anzahl der Samples pro Batch.\n",
    "    - max_length: Maximale Token-Sequenzlänge.\n",
    "    - stride: Schrittweite für die Erzeugung überlappender Sequenzen.\n",
    "    - shuffle: Ob die Reihenfolge der Sequenzen zufällig gemischt wird.\n",
    "    - drop_last: Ob das letzte Batch verworfen wird, falls es unvollständig ist.\n",
    "    - num_workers: Anzahl der Threads für die Datenverarbeitung.\n",
    "\n",
    "    Rückgabe:\n",
    "    - Ein DataLoader-Objekt für das Training.\n",
    "    \"\"\"\n",
    "    # Initialisiere den GPT-2 Tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Erstelle das Dataset\n",
    "    dataset = DatasetForGPT(max_length, stride, tokenizer, txt)\n",
    "\n",
    "    # Erstelle den DataLoader aud der torch Lib (Erklärung 2):\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    #Shuffle sorgt für zufällige Reihenfolge -> Damit das Modell nicht immer dieselbe Reihenfolge der Daten sieht (wichtig für das Training).\n",
    "    # Drop_last vermeidet ungleich große Batches\n",
    "    # Falls die Anzahl der Daten nicht genau durch batch_size teilbar ist, werden unvollständige Batches verworfen.\n",
    "\n",
    "    return dataloader\n"
   ],
   "id": "614b17df2ac83a47"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Erklärung 2:\n",
    "\n",
    "Ein DataLoader bereitet die Daten für das Modelltraining vor. In diesem Fall fasst der Data Loader mehrere Sequenzen (des Sliding Window Approaches) zu Batches zusammen:\n",
    "\n",
    "Das Modell verarbeitet nicht einen einzelnen Satz nach dem anderen, sondern mehrere Sequenzen gleichzeitig (z. B. 4 auf einmal, wenn batch_size=4 ist).\n",
    "###### Beispiel:\n",
    "**Batch 1:**\n",
    "- Eingabe:  `[\"LLMs learn to predict\", \"learn to predict one\", \"to predict one word\", \"predict one word at\"]`\n",
    "- Ziel:     `[\"learn to predict one\", \"to predict one word\", \"predict one word at\", \"one word at a\"]`\n",
    "\n",
    "**Batch 2:**\n",
    "- Eingabe:  `[\"one word at a\", \"word at a time\", \"at a time <|endoftext|>\", \"...\"]`\n",
    "- Ziel:     `[\"word at a time\", \"at a time <|endoftext|>\", \"...\", \"...\"]`\n",
    "\n",
    "\n"
   ],
   "id": "93d083c4261f7d35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.3 Code testen",
   "id": "1c4dc33aaf611bca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(\"test_text\", \"r\") as f:\n",
    "    test_text = f.read()\n",
    "\n",
    "# DataLoader erstellen\n",
    "dataloader = create_dataloader(test_text, batch_size=2, max_length=6, stride=3)\n",
    "\n",
    "# Ersten Batch ausgeben\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(\"\\n=== Erster Batch ===\")\n",
    "    print(\"Input Shape:\", inputs.shape)   # Erwartet: (batch_size, max_length)\n",
    "    print(\"Target Shape:\", targets.shape) # Erwartet: (batch_size, max_length)\n",
    "    print(\"\\nEingabe Batch:\", inputs)\n",
    "    print(\"Ziel Batch:\", targets)\n",
    "    break  # Nur den ersten Batch ausgeben\n",
    "\n"
   ],
   "id": "fcb6789681b915f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1.4 Zusammenfassung Tokenisierung\n",
    "- Das DatasetForGPT erzeugt nur die überlappenden Sequenzen (Sliding Window Approach, siehe Bild 1) und speichert bereits alle Input- und Target Sequenzen\n",
    "- Der DataLoader fasst diese dann in Batches zusammen, die dann im Training verwendet werden können."
   ],
   "id": "65d3c2c3f9d8732a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e96ab94777ba0a4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Sources:\n",
    "\n",
    "Raschka, Sebastian (2025): Build a Large Language Model (from scratch). Shelter Island: Manning (From scratch series). Online verfügbar unter https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=31657639."
   ],
   "id": "c8152daedcd6006b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
