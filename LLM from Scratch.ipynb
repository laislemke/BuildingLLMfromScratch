{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "bdb2f4ba1189940c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import tiktoken  # Tokenizer-Bibliothek für GPT-2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "id": "39c18197745f17a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Tokenisierung:\n",
    "- Tokenisierung ist der Prozess der Umwandlung von Text in kleinere Einheiten (Tokens).\n",
    "- Tokens können Zeichen (\"a\", \"b\"), Wörter (\"Hallo\", \"Baum\") oder Subwörter (\"Ha\", \"Ba\") sein.\n",
    "- LLMs arbeiten mit Token-Sequenzen anstelle von Rohtext → Tokenisierung ist daher das Vorbereiten des Textes für die Eingabe in das LLM :)\n",
    "\n",
    "\n"
   ],
   "id": "821f5ac9591f9c88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1 Datensatz erstellen:",
   "id": "6dccab5cb24e5378"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DatasetForGPT(Dataset):\n",
    "    def __init__(self, max_length, stride, tokenizer, txt):\n",
    "        \"\"\"\n",
    "        Erstellt ein Dataset aus einem Text für ein GPT-basiertes Modell.\n",
    "\n",
    "        Parameter:\n",
    "        - txt: Der Eingabetext als String. (z.B. ein Opensource Buch der Seite Projekt Gutenberg https://www.projekt-gutenberg.org/)\n",
    "        - tokenizer: Der GPT2-Tokenizer zur Tokenisierung des Textes.\n",
    "        - max_length: Maximale Länge einer Token-Sequenz.\n",
    "        - stride: Schrittweite für die Erstellung überlappender Sequenzen.\n",
    "        \"\"\"\n",
    "        self.input_ids = []  # Liste zur Speicherung der Eingabe-Tokens\n",
    "        self.target_ids = []  # Liste zur Speicherung der Ziel-Tokens (verschobene Sequenz)\n",
    "\n",
    "        # Den gesamten Text in Tokens umwandeln\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Erzeuge überlappende Sequenzen aus den Token-IDs (siehe Erklärung 1)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]  # Eingabesequenz\n",
    "            target_chunk = token_ids[\n",
    "                           i + 1: i + max_length + 1]  # Zielsequenz (verschoben um 1 Token -> siehe Bild 1: Erklärung 1 )\n",
    "\n",
    "            # Speichern der Tensor-Repräsentation der Sequenzen\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Gibt die Anzahl der Trainingsbeispiele zurück.\"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Gibt ein Trainingsbeispiel bestehend aus (Eingabe, Ziel) zurück.\"\"\"\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ],
   "id": "6cc5a953243b551d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Erklärung 1\n",
    "Das bedeutet:\n",
    "- Das Modell bekommt eine Eingabesequenz (input_chunk), z. B. \"LLMs learn to predict one\"\n",
    "- Die Zielsequenz (target_chunk) ist dann dieselbe Sequenz, aber um ein Token nach rechts verschoben, sodass das Modell lernen soll, das nächste Token vorherzusagen.\n",
    "\n",
    "→ Genau wie im Bild (Quelle: Raschka 2025)\n",
    "###### Bild 1 Sliding Window Approach\n",
    "![Image 1 Sliding Window Approach](images/Image1_sliding_window_approach.png)\n",
    "\n",
    "Das Modell sieht nur den bisherigen Kontext (blau markiert).\n",
    "Das Modell soll das nächste Wort (rot markiert) vorhersagen.\n",
    "Es kann zukünftige Wörter nicht direkt sehen, sondern muss sie aus den bisherigen Token ableiten. Während des Trainings bekommt das Modell eine Sequenz (input_chunk) als Eingabe und versucht, das nächste Token (target_chunk) vorherzusagen. Durch viele Wiederholungen lernt das Modell dann grammatische Strukturen, Satzbedeutungen und sogar komplexe Zusammenhänge."
   ],
   "id": "968507db476bb9a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Dataloader erzeugen:\n",
    "\n",
    "Der DataLoader hilft, die Daten effizient für das LLM bereitzustelle.\n"
   ],
   "id": "b071da1ed1ccd5e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_dataloader(txt, batch_size=4, max_length=256,\n",
    "                      stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    \"\"\"\n",
    "    Erstellt einen DataLoader für das Training eines LLMs.\n",
    "\n",
    "    Parameter:\n",
    "    - txt: Eingabetext als String.\n",
    "    - batch_size: Anzahl der Samples pro Batch.\n",
    "    - max_length: Maximale Token-Sequenzlänge.\n",
    "    - stride: Schrittweite für die Erzeugung überlappender Sequenzen.\n",
    "    - shuffle: Ob die Reihenfolge der Sequenzen zufällig gemischt wird.\n",
    "    - drop_last: Ob das letzte Batch verworfen wird, falls es unvollständig ist.\n",
    "    - num_workers: Anzahl der Threads für die Datenverarbeitung.\n",
    "\n",
    "    Rückgabe:\n",
    "    - Ein DataLoader-Objekt für das Training.\n",
    "    \"\"\"\n",
    "    # Initialisiere den GPT-2 Tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Erstelle das Dataset\n",
    "    dataset = DatasetForGPT(max_length, stride, tokenizer, txt)\n",
    "\n",
    "    # Erstelle den DataLoader aud der torch Lib (Erklärung 2):\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    #Shuffle sorgt für zufällige Reihenfolge -> Damit das Modell nicht immer dieselbe Reihenfolge der Daten sieht (wichtig für das Training).\n",
    "    # Drop_last vermeidet ungleich große Batches\n",
    "    # Falls die Anzahl der Daten nicht genau durch batch_size teilbar ist, werden unvollständige Batches verworfen.\n",
    "\n",
    "    return dataloader\n"
   ],
   "id": "614b17df2ac83a47"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Erklärung 2:\n",
    "\n",
    "Ein DataLoader bereitet die Daten für das Modelltraining vor. In diesem Fall fasst der Data Loader mehrere Sequenzen (des Sliding Window Approaches) zu Batches zusammen:\n",
    "\n",
    "Das Modell verarbeitet nicht einen einzelnen Satz nach dem anderen, sondern mehrere Sequenzen gleichzeitig (z. B. 4 auf einmal, wenn batch_size=4 ist).\n",
    "###### Beispiel:\n",
    "**Batch 1:**\n",
    "- Eingabe:  `[\"LLMs learn to predict\", \"learn to predict one\", \"to predict one word\", \"predict one word at\"]`\n",
    "- Ziel:     `[\"learn to predict one\", \"to predict one word\", \"predict one word at\", \"one word at a\"]`\n",
    "\n",
    "**Batch 2:**\n",
    "- Eingabe:  `[\"one word at a\", \"word at a time\", \"at a time <|endoftext|>\", \"...\"]`\n",
    "- Ziel:     `[\"word at a time\", \"at a time <|endoftext|>\", \"...\", \"...\"]`\n",
    "\n",
    "\n"
   ],
   "id": "93d083c4261f7d35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3 Code testen",
   "id": "1c4dc33aaf611bca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T18:06:43.484203Z",
     "start_time": "2025-02-03T18:06:43.457297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"test_text\", \"r\") as f:\n",
    "    test_text = f.read()\n",
    "\n",
    "# DataLoader erstellen\n",
    "dataloader = create_dataloader(test_text, batch_size=2, max_length=6, stride=3)\n",
    "\n",
    "# Ersten Batch ausgeben\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(\"\\n=== Erster Batch ===\")\n",
    "    print(\"Input Shape:\", inputs.shape)  # Erwartet: (batch_size, max_length)\n",
    "    print(\"Target Shape:\", targets.shape)  # Erwartet: (batch_size, max_length)\n",
    "    print(\"\\nEingabe Batch:\", inputs)\n",
    "    print(\"Ziel Batch:\", targets)\n",
    "    break  # Nur den ersten Batch ausgeben\n",
    "\n"
   ],
   "id": "fcb6789681b915f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Erster Batch ===\n",
      "Input Shape: torch.Size([2, 6])\n",
      "Target Shape: torch.Size([2, 6])\n",
      "\n",
      "Eingabe Batch: tensor([[ 3436,   351,   683,    13,   314,   550],\n",
      "        [  835,   286,  1762,    30,  2011, 29483]])\n",
      "Ziel Batch: tensor([[  351,   683,    13,   314,   550,  1908],\n",
      "        [  286,  1762,    30,  2011, 29483,  2540]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1.4 Zusammenfassung Tokenisierung\n",
    "- Das DatasetForGPT erzeugt nur die überlappenden Sequenzen (Sliding Window Approach, siehe Bild 1) und speichert bereits alle Input- und Target Sequenzen\n",
    "- Der DataLoader fasst diese dann in Batches zusammen, die dann im Training verwendet werden können."
   ],
   "id": "65d3c2c3f9d8732a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "a6e8e6d0a60d1933"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Attention\n",
    "\n",
    "### 2.1 Warum brauchen wir Attention?\n",
    "Vor der Einführung von **Self-Attention** wurden **Recurrent Neural Networks (RNNs)** und **Encoder-Decoder-Architekturen** für die Verarbeitung von Sequenzen verwendet. Allerdings hatten sie zwei große Probleme:\n",
    "\n",
    "- **Langstreckenabhängigkeiten** wurden nicht gut erfasst, da Informationen aus weit entfernten Tokens schwer erhalten blieben.\n",
    "- **Feste Reihenfolge & Verlust von Kontext** – Die Modelle konnten frühere Tokens nicht direkt abrufen, sondern mussten sich auf eine latente **Hidden State-Repräsentation** verlassen.\n",
    "\n",
    "**Lösung:**\n",
    "**Self-Attention**, ein Mechanismus, der es jedem Token erlaubt, mit allen anderen Tokens in der Sequenz in Beziehung zu treten und zu bestimmen, welche Tokens wichtig sind.\n",
    "\n",
    "\n",
    "### 2.2 Was ist Self-Attention?\n",
    "Self-Attention ist das Herzstück moderner LLMs wie GPT.\n",
    "Es ermöglicht dem Modell, zu jedem Zeitpunkt zu bestimmen, welche Wörter in einem Satz für ein bestimmtes Token am wichtigsten sind. Die „Self“-Komponente in Self-Attention bedeutet, dass das Modell die Aufmerksamkeitsgewichte innerhalb einer einzelnen Eingabesequenz berechnet. Es analysiert die Beziehungen und Abhängigkeiten zwischen verschiedenen Positionen innerhalb der Sequenz selbst – beispielsweise zwischen Wörtern in einem Satz oder Pixeln in einem Bild. Dadurch kann das Modell relevante Informationen aus dem gesamten Kontext ziehen, anstatt sich nur auf benachbarte Elemente zu verlassen.\n",
    "\n",
    "![Image1_Attention_Mechanism](images/Image2_Attention_Mechanisms.png)\n",
    "\n",
    "\n",
    "#### **2.3 Berechnung der Attention-Scores**\n",
    "\n",
    "![Image1_Attention_Mechanism](images/Image3_3stepts.png)\n",
    "\n",
    "\n",
    "\n",
    "#### **2.3.1 Erzeuge Queries, Keys und Values für jedes Token**\n",
    "\n",
    "![Image4](images/Image4_first_step_attention.png)\n",
    "\n",
    "\n",
    "### **Berechnung von Query, Key und Value in Self-Attention**\n",
    "\n",
    "Im ersten Schritt des **Self-Attention-Mechanismus** mit **trainierbaren Gewichtsmatrizen** werden die **Query $(q)$ **, **Key $(k)$** und **Value $(v)$**-Vektoren für die Eingabeelemente $(x)$ berechnet.\n",
    "\n",
    "Ähnlich wie in den vorherigen Abschnitten wird das **zweite Eingabeelement** $(x(2))$ als **Query-Input** ausgewählt. Der Query-Vektor $(q(2))$ wird durch eine **Matrixmultiplikation** zwischen $(x(2))$ und der **Gewichtsmatrix** $(W_Q)$ berechnet:\n",
    "\n",
    "$\n",
    "q(2) = x(2) \\cdot W_Q\n",
    "$\n",
    "\n",
    "Ebenso werden die **Key- und Value-Vektoren** durch Matrixmultiplikation mit den jeweiligen **Gewichtsmatrizen** $(W_K)$ und $(W_V)$ erzeugt:\n",
    "\n",
    "$\n",
    "k(2) = x(2) \\cdot W_K, \\quad v(2) = x(2) \\cdot W_V\n",
    "$\n",
    "\n",
    "#### **2.3.2. Berechnung der Similarity (Attention Scores)**\n",
    "\n",
    "![Image 5](images/Image5_Step2_attention.png)\n",
    "\n",
    "\n",
    "Die Attention Scores ergeben sich aus dem Skalarprodukt zwischen den transformierten Query- und Key-Vektoren.\n",
    "Anstatt einzelne Scores für jedes Token zu berechnen, verwenden wir Matrixmultiplikation, um die Attention Scores für alle Sequenzpositionen gleichzeitig zu berechnen:\n",
    "\n",
    "\n",
    "Die Relevanz eines Tokens für ein anderes wird mit einem **Skalarprodukt** zwischen $(Q)$ und $(K^T)$ berechnet:\n",
    "\n",
    "$\\text{Scores} = Q K^T$\n",
    "\n",
    "Je größer das Ergebnis, desto relevanter ist das Token für den aktuellen Kontext.\n",
    "Da die Dot-Produkte sehr große Werte annehmen können, normalisieren wir sie, indem wir sie durch die Quadratwurzel der Key-Dimension dk teilen (Softmax Funktion).#\n",
    "$\n",
    "\\alpha = \\text{Softmax} \\left(\\frac{Q K^T}{\\sqrt{d_k}} \\right)\n",
    "$\n",
    "\n",
    "---\n",
    "(Skalarproduct:\n",
    "$\n",
    "a \\cdot b = \\sum_{i=1}^{n} a_i b_i\n",
    "$)\n",
    "\n",
    "Beispiel für das Skalarprodukt\n",
    "\n",
    "$\n",
    "a = \\begin{bmatrix} 2 \\\\ 3 \\\\ 4 \\end{bmatrix}, \\quad\n",
    "b = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Das Skalarprodukt wird berechnet als:\n",
    "\n",
    "$\n",
    "a \\cdot b = (2 \\cdot 1) + (3 \\cdot 0) + (4 \\cdot (-1))\n",
    "$\n",
    "\n",
    "$\n",
    "a \\cdot b = 2 + 0 - 4 = -2\n",
    "$\n",
    "\n",
    "Ergebnis: Das Skalarprodukt von $(a)$ und $(b)$** ist $(-2)$.\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### **2.3.3. Softmax zur Normalisierung der Gewichte**\n",
    "![Image 6](images/Image6_step3_attention.png)\n",
    "\n",
    "Softmax zur Normalisierung der Scores in Wahrscheinlichkeiten\n",
    "Nachdem die Attention-Scores $(\\omega)$ berechnet wurden, werden diese mithilfe der Softmax-Funktion normalisiert,\n",
    "um die Attention-Gewichte $(\\alpha)$ zu erhalten.\n",
    "Dabei wird sichergestellt, dass alle Attention-Gewichte positiv sind und ihre Summe 1 ergibt.\n",
    "\n",
    "$\\alpha = \\text{Softmax} \\left(\\frac{QK^T}{\\sqrt{d_k}} \\right)$\n",
    "\n",
    "\n",
    "#### **4. Erzeuge Kontextvektoren als gewichtete Summe der Values**\n",
    "Schließlich werden die normalisierten Scores genutzt, um die **Values** zu gewichten:\n",
    "\n",
    "$Z = \\alpha V$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.4 Causal Attention für LLMs\n",
    "GPT-Modelle nutzen eine spezielle Form der Self-Attention, **Causal Attention**, um sicherzustellen, dass ein Modell bei der **Textgenerierung nur auf frühere Tokens schaut**.\n",
    "\n",
    "**Wie wird das erreicht?**\n",
    "- Eine **Causal Mask** wird auf die Attention Scores angewendet, um **zukünftige Tokens auszublenden**.\n",
    "- Mathematisch bedeutet das, dass Werte oberhalb der Hauptdiagonale der Attention-Matrix mit **\\(-\\infty\\)** belegt werden.\n",
    "\n",
    "\n",
    "\n",
    "### 2.5 Multi-Head Attention – Erweiterung von Self-Attention\n",
    "Anstatt nur **einen einzigen Self-Attention-Mechanismus** zu nutzen, **wird dieser in mehrere parallele \"Köpfe\" (Heads) aufgeteilt**.\n",
    "\n",
    "### **Warum Multi-Head Attention?**\n",
    "**Lernt verschiedene Aspekte einer Sequenz gleichzeitig**\n",
    "- Jeder Kopf fokussiert sich auf **unterschiedliche Muster** (Syntax, Semantik, Langstreckenabhängigkeiten).\n",
    "\n",
    "**Verbessert die Stabilität des Trainings**\n",
    "- Mehrfache unabhängige Berechnungen gleichen mögliche Fehler einzelner Heads aus.\n",
    "\n",
    "**Erhöht die Repräsentationskraft des Modells**\n",
    "- Durch die Kombination mehrerer Perspektiven erhält das Modell **tiefere Einsichten** in die Bedeutung eines Satzes.\n",
    "\n",
    "\n",
    "\n",
    "### **Technische Umsetzung**\n",
    "- **Jeder Kopf hat eigene Gewichtsmatrizen** $(W_Q, W_K, W_V)$\n",
    "- **Die Self-Attention wird parallel für jeden Kopf berechnet.**\n",
    "- Danach werden die Ergebnisse aller Heads **konkateniert** und durch eine weitere Projektion zusammengeführt:\n",
    "\n",
    "$Z_{\\text{final}} = W_O [Z_1, Z_2, ..., Z_h]$\n",
    "\n",
    "wobei $(W_O)$ eine weitere Gewichtsmatrix ist, die das gesamte Ergebnis transformiert.\n",
    "\n",
    "\n",
    "\n",
    "### 2.6 Unterschiedliche Implementierungen von Multi-Head Attention\n",
    "Rashka (2024) beschreibt zwei Implementierungsansätze:\n",
    "\n",
    "#### **Einfache Multi-Head Attention durch Stacken von mehreren Attention-Modulen**\n",
    "Mehrere **unabhängige Self-Attention-Blöcke** werden nebeneinander gelegt und deren Ergebnisse kombiniert.\n",
    "**Nachteil**: Rechenaufwändig, da jede Attention **separat** berechnet wird.\n",
    "\n",
    "#### **Effiziente Multi-Head Attention mit paralleler Berechnung**\n",
    "**Ein einziger linearer Layer** erzeugt **Queries, Keys und Values** für alle Heads gleichzeitig.\n",
    "Diese werden dann **in verschiedene Heads gesplittet**, sodass die Berechnungen **parallel stattfinden können**.\n",
    "**Vorteil**: Spart **Speicher** und ist **rechenoptimiert**.\n",
    "\n",
    "\n",
    "#### Dropout in Attention – Reduzierung von Overfitting\n",
    "Zusätzlich zu **Causal Masks** wird **Dropout auf die Attention Scores angewendet**, um:\n",
    "- **Overfitting zu vermeiden**, indem zufällige Gewichte auf 0 gesetzt werden.\n",
    "- **Das Modell zu zwingen, robustere Muster zu lernen.**"
   ],
   "id": "77200f55428c575d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        \"\"\"\n",
    "        Implementiert Multi-Head Attention, ein zentraler Mechanismus in Transformer-Modellen.\n",
    "\n",
    "        Parameter:\n",
    "        - d_in: Eingabe-Dimension (z. B. Anzahl der Merkmale pro Token).\n",
    "        - d_out: Ausgabe-Dimension (muss durch num_heads teilbar sein).\n",
    "        - context_length: Maximale Sequenzlänge.\n",
    "        - dropout: Dropout-Wahrscheinlichkeit zur Regularisierung.\n",
    "        - num_heads: Anzahl der parallelen Attention-Heads.\n",
    "        - qkv_bias: Ob die linearen Projektionsmatrizen für Q, K und V einen Bias enthalten.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out muss durch num_heads teilbar sein\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Dimension jedes Attention-Heads\n",
    "\n",
    "        # Lineare Transformationen für Query, Key und Value\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # Finale lineare Projektion nach der Attention-Berechnung\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Maskierung für zukünftige Tokens (Causal Mask für autoregressive Modelle)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Führt den Multi-Head Attention Mechanismus aus.\n",
    "        - x: Eingabe-Tensor der Form (Batch, Sequenzlänge, d_in)\n",
    "        \"\"\"\n",
    "        b, num_tokens, d_in = x.shape  # Batch-Größe, Sequenzlänge, Eingabe-Dimension\n",
    "\n",
    "        # Berechnung von Query, Key und Value\n",
    "        keys = self.W_key(x)  # (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Aufteilen in mehrere Köpfe (num_heads)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1,\n",
    "                                                                                 2)  # (b, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Berechnung der Attention Scores mittels Skaliertem Dot-Product Attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = attn_scores / (self.head_dim ** 0.5)  # Skalierung für stabilere Gradienten\n",
    "\n",
    "        # Anwenden der Maskierung, um zukünftige Tokens auszublenden (Causal Masking)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # Softmax zur Normalisierung der Scores in Wahrscheinlichkeiten\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Berechnung der gewichteten Summen der Values\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Rekombinieren der Heads zur ursprünglichen Ausgabe-Dimension\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # Finale lineare Transformation\n",
    "\n",
    "        return context_vec\n"
   ],
   "id": "85c98b68cdda1fc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "904fec100142c02b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Sources:\n",
    "\n",
    "Raschka, Sebastian (2025): Build a Large Language Model (from scratch). Shelter Island: Manning (From scratch series). Online verfügbar unter https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=31657639."
   ],
   "id": "c8152daedcd6006b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
