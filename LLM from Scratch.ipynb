{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb2f4ba1189940c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39c18197745f17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tiktoken  # Tokenizer-Bibliothek für GPT-2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821f5ac9591f9c88",
   "metadata": {},
   "source": [
    "# 1. Tokenisierung:\n",
    "- Tokenisierung ist der Prozess der Umwandlung von Text in kleinere Einheiten (Tokens).\n",
    "- Tokens können Zeichen (\"a\", \"b\"), Wörter (\"Hallo\", \"Baum\") oder Subwörter (\"Ha\", \"Ba\") sein.\n",
    "- LLMs arbeiten mit Token-Sequenzen anstelle von Rohtext → Tokenisierung ist daher das Vorbereiten des Textes für die Eingabe in das LLM :)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccab5cb24e5378",
   "metadata": {},
   "source": [
    "#### 1.1 Datensatz erstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cc5a953243b551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForGPT(Dataset):\n",
    "    def __init__(self, max_length, stride, tokenizer, txt):\n",
    "        \"\"\"\n",
    "        Erstellt ein Dataset aus einem Text für ein GPT-basiertes Modell.\n",
    "\n",
    "        Parameter:\n",
    "        - txt: Der Eingabetext als String. (z.B. ein Opensource Buch der Seite Projekt Gutenberg https://www.projekt-gutenberg.org/)\n",
    "        - tokenizer: Der GPT2-Tokenizer zur Tokenisierung des Textes.\n",
    "        - max_length: Maximale Länge einer Token-Sequenz.\n",
    "        - stride: Schrittweite für die Erstellung überlappender Sequenzen.\n",
    "        \"\"\"\n",
    "        self.input_ids = []  # Liste zur Speicherung der Eingabe-Tokens\n",
    "        self.target_ids = []  # Liste zur Speicherung der Ziel-Tokens (verschobene Sequenz)\n",
    "\n",
    "        # Den gesamten Text in Tokens umwandeln\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Erzeuge überlappende Sequenzen aus den Token-IDs (siehe Erklärung 1)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]  # Eingabesequenz\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]  # Zielsequenz (verschoben um 1 Token -> siehe Bild 1: Erklärung 1 )\n",
    "\n",
    "            # Speichern der Tensor-Repräsentation der Sequenzen\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Gibt die Anzahl der Trainingsbeispiele zurück.\"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Gibt ein Trainingsbeispiel bestehend aus (Eingabe, Ziel) zurück.\"\"\"\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968507db476bb9a3",
   "metadata": {},
   "source": [
    "##### Erklärung 1\n",
    "Das bedeutet:\n",
    "- Das Modell bekommt eine Eingabesequenz (input_chunk), z. B. \"LLMs learn to predict one\"\n",
    "- Die Zielsequenz (target_chunk) ist dann dieselbe Sequenz, aber um ein Token nach rechts verschoben, sodass das Modell lernen soll, das nächste Token vorherzusagen.\n",
    "\n",
    "→ Genau wie im Bild (Quelle: Raschka 2025)\n",
    "###### Bild 1 Sliding Window Approach\n",
    "![Image 2 Sliding Window Approach](images/Image1_sliding_window_approach.png)\n",
    "\n",
    "Das Modell sieht nur den bisherigen Kontext (blau markiert).\n",
    "Das Modell soll das nächste Wort (rot markiert) vorhersagen.\n",
    "Es kann zukünftige Wörter nicht direkt sehen, sondern muss sie aus den bisherigen Token ableiten. Während des Trainings bekommt das Modell eine Sequenz (input_chunk) als Eingabe und versucht, das nächste Token (target_chunk) vorherzusagen. Durch viele Wiederholungen lernt das Modell dann grammatische Strukturen, Satzbedeutungen und sogar komplexe Zusammenhänge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071da1ed1ccd5e2",
   "metadata": {},
   "source": [
    "#### 1.2 Dataloader erzeugen:\n",
    "\n",
    "Der DataLoader hilft, die Daten effizient für das LLM bereitzustelle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "614b17df2ac83a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, batch_size=4, max_length=256,\n",
    "                      stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    \"\"\"\n",
    "    Erstellt einen DataLoader für das Training eines LLMs.\n",
    "\n",
    "    Parameter:\n",
    "    - txt: Eingabetext als String.\n",
    "    - batch_size: Anzahl der Samples pro Batch.\n",
    "    - max_length: Maximale Token-Sequenzlänge.\n",
    "    - stride: Schrittweite für die Erzeugung überlappender Sequenzen.\n",
    "    - shuffle: Ob die Reihenfolge der Sequenzen zufällig gemischt wird.\n",
    "    - drop_last: Ob das letzte Batch verworfen wird, falls es unvollständig ist.\n",
    "    - num_workers: Anzahl der Threads für die Datenverarbeitung.\n",
    "\n",
    "    Rückgabe:\n",
    "    - Ein DataLoader-Objekt für das Training.\n",
    "    \"\"\"\n",
    "    # Initialisiere den GPT-2 Tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Erstelle das Dataset\n",
    "    dataset = DatasetForGPT(max_length, stride, tokenizer, txt)\n",
    "\n",
    "    # Erstelle den DataLoader aud der torch Lib (Erklärung 2):\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    #Shuffle sorgt für zufällige Reihenfolge -> Damit das Modell nicht immer dieselbe Reihenfolge der Daten sieht (wichtig für das Training).\n",
    "    # Drop_last vermeidet ungleich große Batches\n",
    "    # Falls die Anzahl der Daten nicht genau durch batch_size teilbar ist, werden unvollständige Batches verworfen.\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d083c4261f7d35",
   "metadata": {},
   "source": [
    "##### Erklärung 2:\n",
    "\n",
    "Ein DataLoader bereitet die Daten für das Modelltraining vor. In diesem Fall fasst der Data Loader mehrere Sequenzen (des Sliding Window Approaches) zu Batches zusammen:\n",
    "\n",
    "Das Modell verarbeitet nicht einen einzelnen Satz nach dem anderen, sondern mehrere Sequenzen gleichzeitig (z. B. 4 auf einmal, wenn batch_size=4 ist).\n",
    "###### Beispiel:\n",
    "**Batch 1:**\n",
    "- Eingabe:  `[\"LLMs learn to predict\", \"learn to predict one\", \"to predict one word\", \"predict one word at\"]`\n",
    "- Ziel:     `[\"learn to predict one\", \"to predict one word\", \"predict one word at\", \"one word at a\"]`\n",
    "\n",
    "**Batch 2:**\n",
    "- Eingabe:  `[\"one word at a\", \"word at a time\", \"at a time <|endoftext|>\", \"...\"]`\n",
    "- Ziel:     `[\"word at a time\", \"at a time <|endoftext|>\", \"...\", \"...\"]`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4dc33aaf611bca",
   "metadata": {},
   "source": [
    "#### 1.3 Code testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcb6789681b915f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Erster Batch ===\n",
      "Input Shape: torch.Size([2, 6])\n",
      "Target Shape: torch.Size([2, 6])\n",
      "\n",
      "Eingabe Batch: tensor([[  257,  2726,  6227,   284,  1833,   683],\n",
      "        [ 1534,  4241,   286, 19217,   290, 18876]])\n",
      "Ziel Batch: tensor([[ 2726,  6227,   284,  1833,   683,  1365],\n",
      "        [ 4241,   286, 19217,   290, 18876,  5563]])\n"
     ]
    }
   ],
   "source": [
    "with open(\"test_text\", \"r\") as f:\n",
    "    test_text = f.read()\n",
    "\n",
    "# DataLoader erstellen\n",
    "dataloader = create_dataloader(test_text, batch_size=2, max_length=6, stride=3)\n",
    "\n",
    "# Ersten Batch ausgeben\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    print(\"\\n=== Erster Batch ===\")\n",
    "    print(\"Input Shape:\", inputs.shape)   # Erwartet: (batch_size, max_length)\n",
    "    print(\"Target Shape:\", targets.shape) # Erwartet: (batch_size, max_length)\n",
    "    print(\"\\nEingabe Batch:\", inputs)\n",
    "    print(\"Ziel Batch:\", targets)\n",
    "    break  # Nur den ersten Batch ausgeben\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d3c2c3f9d8732a",
   "metadata": {},
   "source": [
    "#### 1.4 Zusammenfassung Tokenisierung\n",
    "- Das DatasetForGPT erzeugt nur die überlappenden Sequenzen (Sliding Window Approach, siehe Bild 1) und speichert bereits alle Input- und Target Sequenzen\n",
    "- Der DataLoader fasst diese dann in Batches zusammen, die dann im Training verwendet werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89a999",
   "metadata": {},
   "source": [
    "**Multi-Head Attention aus Kapitel 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3b360ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ea243",
   "metadata": {},
   "source": [
    "# 4. Implemetierung des GPT-Modells:\n",
    "- Konfiguration des GPT-Modells.\n",
    "- Die Architektur ist wie folgt aufgebaut:\n",
    "    - Grundgerüst der Architektur, Layer normalization, GELU-Aktivierung, Feed forward network, Shortcut connections, Transformer block.\n",
    "- Transformer blocks sind eine zentrale strukturelle Komponente von GPT-Modellen - Diese kombinieren masked multi-head attentions Module mit vollständig verbundenen feed forward networks, die die GELU activation function nutzen\n",
    "###### Bild 3 Visualisierung der Architektur\n",
    "![Image 3 Visualisierung der Architektur](images/LLM-Architektur.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbe11e",
   "metadata": {},
   "source": [
    "#### 4.1 Eine LLM-Architektur programmieren\n",
    "\n",
    "**Config des GPT-Modells**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9693cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257, # Vokabular an Wörtern - Byte Pair Encoding (BPE)-Tokenizer\n",
    "\"context_length\": 1024, # Maximale Anzahl an Input-Tokens\n",
    "\"emb_dim\": 768, # Embedding size - transformiert jedes Token in ein 768-dimensionalen Vektor\n",
    "\"n_heads\": 12, # Anzahl der attention heads\n",
    "\"n_layers\": 12, # Anzahl der Transformer-Blöcke\n",
    "\"drop_rate\": 0.1, # Dropout Mechanismus - Overfitting vermeiden durch dropouts (0.1 sind somit 10% zufülliger dropout)\n",
    "\"qkv_bias\": False # Bestimmt ob ein Bias-Vektor in der linearen Layer der Multi-head-atention beinhaltet sein soll\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d583525",
   "metadata": {},
   "source": [
    "Zur Implementierung wird ein batch tokenisiert der aus zwei Text-Inputs für das GPT-Modell besteht.\n",
    "\n",
    "! *Es wird hierfür der Tiktoken-Tokenizer aus Kapitel 2 verwendet* !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45ac292",
   "metadata": {},
   "source": [
    "##### 4.2 Normalizing Activation mit Layer normalization\n",
    "- Hauptidee der Layer normalization besteht in der Anpassung der Activations (Outputs) einer neural network layer. Es soll somit einen Mittelwert von 0 und eine Varianz von 1 haben (Unit Variance)\n",
    "- Beschleunigt die Konvergenz zu effektiven Gewichten und gewährleistet ein konistentes und zuverlässiges Training.\n",
    "- Die Layer normalization wird in modernen Transformer-Architekturen in der Regel vor und nach dem Multi-Head-Attention-Modul angewendet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4cce563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim): #emb_dim = embedding dimension (letze Dimension des input tensors x / Größe der Vektoren, die die Token im Modell repräsentieren)\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5          # Dadurch wird vermieden, dass es zu einer Division durch 0 kommt, falls die Varianz extrem klein oder genau 0 ist.\n",
    "\n",
    "        # Nach der Normalisierung wären alle Werte auf eine Standardnormalverteilung mit Mittelwert = 0 und Varianz = 1 gebracht\n",
    "        # Damit das Modell aber flexibel bleibt, gibt es zwei trainierbare Parameter:\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim)) # Multiplikationsfaktor, der die Werte nach der Normalisierung wieder streckt oder komprimiert.\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) # Ein Wert, der hinzugefügt wird, um die Normalisierung nach oben oder unten zu verschieben.\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # \n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b1f0d",
   "metadata": {},
   "source": [
    "**Testen und Darstellung der Layer normalization anhand eines Beispiels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35b2dfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erstellung von 2 trainings-beispielen mit fünf Dimensionen: \n",
      "\n",
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>) \n",
      "\n",
      "Mittelwert und Varianz ohne Layer normalization: \n",
      "\n",
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>) \n",
      "\n",
      "Layer normalization mit batch-input: \n",
      "\n",
      "Mean:\n",
      " tensor([[-2.9802e-08],\n",
      "        [ 0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Erstellung von 2 trainings-beispielen mit fünf Dimensionen\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(\"Erstellung von 2 trainings-beispielen mit fünf Dimensionen: \\n\")\n",
    "print(out, \"\\n\")\n",
    "\n",
    "# Mittelwert und Varianz ohne Layer normalization:\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mittelwert und Varianz ohne Layer normalization: \\n\")\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var, \"\\n\")\n",
    "\n",
    "\n",
    "# Layer normalization mit batch-input\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Layer normalization mit batch-input: \\n\")\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e03ed",
   "metadata": {},
   "source": [
    "##### 4.3 Implementierung eines feed forward network mit GELU activations\n",
    "- GELU (Gaussian error linear unit)\n",
    "- Kleines neuronales Netzwerk Submodule welches als Teil des transformer blocks in LLMs genutzt wird.\n",
    "- Aktivierungsfunktionen, die Gaußsche bzw. sigmoidale lineare Einheiten enthalten und bieten eine verbesserte Leistung für Deep-Learning-Modelle.\n",
    "\n",
    "*Funktion kann als PyTorch module implementiert werden*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cd4e25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor ist der selbe wie im input Tensor: \n",
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "# Wird später im Transformer Block verwendet\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "# FeedForward module mit einer Token embedding size von 768 und feed in einen batch input mit zwei samples und jeweils drei Token     \n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(\"Output Tensor ist der selbe wie im input Tensor: \")\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632a1ba",
   "metadata": {},
   "source": [
    "##### 4.4 Shortcut Connections\n",
    "- Werden auch skip or residual connections genannt.\n",
    "- Verminderung der Herausforderung des vanishing gradients (Gradienten werden immer kleiner was es schwierig macht, frühere Layers effektiv zu trainieren).\n",
    "- Gradienten \"leiten\" die updates der Gewichte während des Trainings.  \n",
    "\n",
    "*Ermöglichen einen alternativen kürzeren Weg für den Gradienten der durch das Netz fließt indem eine oder mehrere Schichten übersprungen werden indem der Output einer Layer zum Oustput einer späteren layer hinzugefügt wird - Wichtige Rolle während des backward pass im Training*\n",
    "\n",
    "###### Bild 4 Visualisierung Shortcut Connection\n",
    "![Image 4 Visualisierung Shortcut Connection](images/Shortcut-Connection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b374394",
   "metadata": {},
   "source": [
    "##### 4.5 Verbinden von Attention und Linearen Layers in einem Transformer-Block\n",
    "- Der Transformer-Block kombiniert die bereits thematisierten Konzepte wie Multi-Head-Attemtion, layer normalization, dropout, feed forward layers und GELU activations.\n",
    "- Die Operationen innerhalb des Transformer-Blocks, einschließlich der Multi-Head-Attention- und Feed-Forward-Layers sind darauf ausgelegt, die Vektoren so zu transformieren, dass ihre Dimensionalität erhalten bleibt.\n",
    "- Die Idee ist, dass der Self-Attention-Mechanism im Multi-Head-Attention-Block Beziehungen zwischen Elementen in der Input-Sequenz identifiziert und analysiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "100396f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut Connection für den attention-block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   \n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # Den orginalen input hinzufügen\n",
    "\n",
    "        # Shortcut connection für den feed-forward-block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # Den orginalen input hinzufügen\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b096776d",
   "metadata": {},
   "source": [
    "**Testen und Darstellung des Transformer-Blocks anhand eines Beispiels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4422d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformer-Block mit Beispieldaten\n",
    "# Dieser behält Input Dimensionen im Output und zeigt, dass die Transformer-Architektur \n",
    "# Datenfolgen verarbeitet, ohne ihre Form im gesamten Netzwerk zu verändern.\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1defe952",
   "metadata": {},
   "source": [
    "##### 4.6 Das GPT-Modell coden\n",
    "Zusammenführung der zuvor thematisierten Punkte\n",
    "\n",
    "###### Bild 5 Visualisierung Datenfluss der GPT-Modell-Architektur\n",
    "![Image 5 Visualisierung Datenfluss der GPT-Modell-Architektur](images/GPT-Modell-Architektur.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6340d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg): # init constructor initialisiert die Token- und positional embedding Layers durch die Übergabe der Konfiigurationen aus des Python dictionary (cfg)\n",
    "        super().__init__()  # Embeddings sind für die Umwandlung der eingegebenen Token-Indizes in dichte Vektoren und das Hinzufügen von Positionsinformationen verantwortlich\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Erstellung sequenzeller Stapel (stack) von Transformer-Blöcken. Gleiche Anzahl wie die definierten Layers in cfg.\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        # LayerNorm (Outputs der Transformer-Blöcke standardisieren, um den Lernprozess zu stabilisieren)\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    # Nimmt einen batch an input-token-indizes, berechnet deren embeddings, wendet positional embeddings an, \n",
    "    # leitet die Sequenz durch die Transformer-Blöcke, normalisiert den Output und berechnet dann die Logits, \n",
    "    # die die nicht normalisierten Wahrscheinlichkeiten des nächsten Tokens darstellen\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a4051",
   "metadata": {},
   "source": [
    "**weight tying**\n",
    "- 163M statt 124M Parameter\n",
    "- Wurde in der originalen GPT-2 Architektur genutzt - Wiederverwendung die Gewichte der Token embedding Layer in der Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb7bd49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n",
      "Number of trainable parameters considering weight tying: 124,412,160\n",
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "total_params_gpt2 = (\n",
    "total_params - sum(p.numel()\n",
    "for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {total_params_gpt2:,}\"\n",
    ")\n",
    "\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6aebe",
   "metadata": {},
   "source": [
    "##### 4.7 Text generieren\n",
    "- Implementierung einer generativen Schleife für ein Language Model mit PyTorch\n",
    "- Durchläuft eine bestimmte Anzahl neu zu erzeugender Token und berechnet Vorhersagen und wählt dann das nächste Token auf der Grundlage der Vorhersage mit der höchsten Wahrscheinlichkeit aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da76038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx (Batch & Tokens) ist ein Array von Indizes im aktuellen context \n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Cropt den aktuellen Kontext wenn das LLM bspw. nur 5 tokens unterstützt, der Context aber 10 ist, werden nur die letzten 5 Tokens als Context genutzt\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Vorhersagen bekommen\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Fokussiert sich auf den letzten Schritt\n",
    "        # (batch, n_token, vocab_size) wird zu (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Idx mit dem höchsten logits wert\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Anhängen des sampled index an die laufende Sequenz\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc72700",
   "metadata": {},
   "source": [
    "**Testen und Darstellung der Textgeneration anhand eines Beispiels**\n",
    "\n",
    "*Modell kann noch keinen kohärenten Text produzieren, da es noch nicht trainiert ist*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "507f1582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "Output: tensor([[15496,    11,   314,   716, 21203, 47522, 12355, 29196, 20122, 45721]])\n",
      "Output length: 10\n",
      "Hello, I am VictoryFacjava directoriesheartedAPE\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8152daedcd6006b",
   "metadata": {},
   "source": [
    "#### Sources:\n",
    "\n",
    "Raschka, Sebastian (2025): Build a Large Language Model (from scratch). Shelter Island: Manning (From scratch series). Online verfügbar unter https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=31657639."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
